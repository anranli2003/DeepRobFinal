
<!doctype html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Learning </title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="PerceptFusion: Real-Time Object Understanding Through Multi-Modal Perceptual Integration" />
<meta name="author" content="Parker Ewen" />
<meta property="og:locale" content="en" />
<meta name="description" content="PerceptFusion: Real-Time Object Understanding Through Multi-Modal Perceptual Integration." />
<meta property="og:description" content="Real-time object understanding through pose estimation, semantic segmentation, and hand-eye calibrated perception" />
<link rel="canonical" href="https://anranli2003.github.io/DeepRobFinal/" />
<meta property="og:url" content="https://roahmlab.github.io/multimodal_mapping/" />
<meta property="og:site_name" content="PerceptFusion" />
<meta property="og:image" content="https://raw.githubusercontent.com/ParkerEwen5441/github.io-multimodal_mapping/main/web_elements/pitch.png" />
<meta property="og:image:height" content="100" />
<meta property="og:image:width" content="256" />
<meta property="og:image:alt" content="Random Landscape" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-02-08T17:24:00+00:00" />


<link rel="canonical" href="https://anranli2003.github.io/DeepRobFinal/" /><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&family=Cabin:wght@300;400;700&display=swap" rel="stylesheet"><link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;700&family=Noto+Sans+KR:wght@400;700&family=Noto+Sans+SC:wght@400;700&display=swap" rel="stylesheet"><link rel="stylesheet" href="css/stylesheet.css">

  
    
  
    
  
    
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body><header class="page-header" role="banner">
  <div class="wrapper">
    <a class="page-title" rel="bookmark" href="https://anranli2003.github.io/DeepRobFinal/">PerceptFusion: Real-Time Object Understanding Through Multi-Modal Perceptual Integration</a>
    <div class="page-description"><p>Real-time object understanding through pose estimation, semantic segmentation, and hand-eye calibrated perception</p>
    </div>
</div>
</header>
<main class="content" aria-label="Content">
      <div class="wrapper"><div class="authors">
  <div class="box">
    <div class="single">
      <p class="name">Yuzhen Chen</p>
        <p class="email">yuzhench@umich.edu</p>
    </div>
    <div class="single">
      <p class="name">Yeheng Zong</p>
        <p class="email">yehengz@umich.edu</p>
    </div>
    <div class="single">
      <p class="name">Anran Li</p>
        <p class="email">anranli@umich.edu</p>
    </div>
    <div class="single">
      <p class="name">Jiamu Liu</p>
        <p class="email">dliujm@umich.edu</p>
    </div>
  </div>
  <div class="footnote">
  
    <p>All authors affiliated with the Robotics Institute and department of Mechanical Engineering of the University of Michigan, Ann Arbor.</p>

  
  </div>
</div>

<div class="external-links">
  <div class="box">
    <div class="single">
      <div class="icon"><i class="si-wrapper" style="background-image: url(https://cdn.jsdelivr.net/npm/simple-icons@v9/icons/arxiv.svg)"></i></div><p>Report</p><a href="https://arxiv.org/abs/2402.05872"></a></div>
    <div class="single">
      <div class="icon"><i class="si-wrapper" style="background-image: url(https://cdn.jsdelivr.net/npm/simple-icons@v9/icons/github.svg)"></i></div><p>Code</p><a href="https://github.com/yuzhench/EECS498_004-final-project-"></a></div>
  </div>
</div>

<!-- # [Content](#content)
<div markdown="1" class="content-block grey justify no-pre">
some text

Try clicking this heading, this shows the manually defined header anchor, but if you do this, you should do it for all headings.
</div>

I made this look right by adding the `no-pre` class.
If you don't include `markdown="1"` it will fail to render any markdown inside.

You can also make fullwidth embeds (this doesn't actually link to any video)
 -->
<h1 id="demo-video">
  
  
    Demo Video <a href="#demo-video" class="permalink">#</a>
  
  
</h1>
    
<div class="fullwidth">
<video controls="" width="100%">
    <source src="https://github.com/anranli2003/DeepRobFinal/tree/master/assets/rob498demo.mp4" />
</video>
</div>

<div class="content-block grey justify">
<h1 id="abstract">
  
  
    Abstract <a href="#abstract" class="permalink">#</a>
  
  
</h1>
    

  <p>In this project, we present a real-time, language-guided 6D object pose estimation pipeline that integrates modern vision-language models with classical 3D geometric techniques. Our pipeline begins with a fine-tuned Grouding-DINO model for language-guided object localization and a fine-tuned Segment Anything Model 2 (SAM2) for instance segmentation. To achieve accurate 3D reconstruction, we performed extrinsic calibration between an Azure Kinect RGB-D camera and a motion capture system. Point cloud data are generated from the depth information provided by Kinect, from which points belonging to the target object are selected by back-projecting the segmented 2D image region. This is followed by key-point extraction, FPFH-based correspondence matching, and coarse pose estimation using RANSAC-based algorithms. The final pose refinement is then obtained through an Iterative Closest Point (ICP) algorithm. We validate the full pipeline through experiments and report pose estimation accuracy and performance rate, demonstrating the system's ability to operate effectively in real-time. </p>

  <p align="center">
<img src="assets/IMG_8013.jpg" class="img-responsive" alt="" width="500" height="500" />
</p>

 </div>
<h1 id="method">
  
  
    Method <a href="#method" class="permalink">#</a>
  
  
</h1>
    

<p>A flow diagram illustrating our proposed mutli-modal perceptual integration.
  The pipeline follows a layered structure moving from language to image, point cloud, and finally pose. It begins with sensor setup and extrinsic calibration using an Azure Kinect, AprilTag, and optical motion capture, solving camera extrinsics via a Procrustes optimization. A language command is processed by a fine-tuned Grounding-DINO model to localize a 2D target, followed by instance segmentation with a domain-adapted SAM model to generate dense 3D point clouds. Finally, key points are extracted, descriptors are computed, and coarse point cloud registration is performed for pose estimation.</p>

  <p><img src="assets/pipeline.png" alt="Flow diagram for multi-modal perceptual integration" title="Flow Diagram" /></p>

  <!-- <p></p> -->
  
  <div class="content-block grey justify">
    <h1 id="experiment-results">
        Experiment Results <a href="#experiment-results" class="permalink">#</a>
    </h1>
    
    <p>To demonstrate the real-world performance of our proposed pipeline, we established a custom experimental workspace and collected our own dataset for fine-tuning and evaluation. Our setup included an Azure Kinect camera positioned in front of a workspace, a Kinova robotic arm with an AprilTag mounted to its end-effector, and an activated OptiTrack motion capture system for precise ground truth collection. We performed hand-eye calibration by moving the arm to different positions. Three distinct target objects—a square, circle, and triangle—were suspended at varying positions, each marked with at least three fiducial markers.</p>
    
    <p>For data preparation, we recorded synchronized RGB-D sequences and 3D trajectories. Although the Azure Kinect and OptiTrack systems were activated roughly at the same time, we performed fine synchronization using key event matching and addressed the difference in frame rates (30 fps vs. 100 fps) with a bidirectional time index mapping. To annotate masks for fine-tuning SAMv2, we leveraged color contrast for coarse masks and refined them using depth thresholding, isolating individual objects cleanly even when parts of the Kinova arm overlapped. With precise masks and bounding boxes, we prepared high-quality training data for fine-tuning GroundingDINO and SAMv2 for our target domain.</p>
    
    <p><img src="assets/results.png" alt="Experiment setup for real-world fine-tuning" title="Experimental Setup" /></p>
    
    <p>We fine-tuned GroundingDINO using open-source guidelines and further fine-tuned SAMv2 using our annotated dataset. The results demonstrate that our approach enables more accurate localization and segmentation of objects under real-world conditions, validating the effectiveness of the proposed layered pipeline that progresses from language input to 3D pose estimation.</p>
    
    </div>
    
    <h1 id="hardware-demonstrations">
        Hardware Demonstrations <a href="#hardware-demonstrations" class="permalink">#</a>
    </h1>
    
    <p>We further validated our method through hardware demonstrations using the Kinova Gen 3 robotic arm. After fine-tuning, we evaluated the system by issuing language commands to detect and localize specified target objects in the workspace. The system successfully processed language input, localized objects in 2D through GroundingDINO, generated instance masks with SAMv2, and lifted 2D segmentations to 3D using depth information. We registered these lifted points with the ground-truth poses obtained from the OptiTrack system, enabling precise localization of target objects for robotic manipulation.</p>
    
    <p><img src="assets/hardware_results.png" alt="Hardware demonstration results" title="Hardware Demonstrations" /></p>
    
    <p>These experiments validate that our multi-modal pipeline can operate effectively with minimal supervision, accurately linking natural language queries to real-world 3D object poses. Our approach demonstrates robustness even when facing noisy depth readings or minor visual ambiguities, providing a practical pathway for language-driven robotic perception and manipulation tasks.</p>
    

    

<div class="content-block grey justify">
<h1 id="citation">
  
  
    Citation <a href="#citation" class="permalink">#</a>
  
  
</h1>
    

  <p>This project was developed as a part of the Deep Learning for Robot Perception course at the University of Michigan Robotics Department.</p>

  <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl"></span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">"Chen, Yuzhen and Zong, Yeheng and Li, Anran and Liu, Jiamu"</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">"PerceptFusion: Real-Time Object Understanding Through Multi-Modal Perceptual Integration"</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="m">2025</span>
<span class="p">}</span>
</code></pre></div>  </div>
</div>

      </div>
    </main>

</body>
</html>

